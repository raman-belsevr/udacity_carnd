{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ML pipeline\n",
    "## DataSource -> Pre-Process -> InputData\n",
    "## TrainingModule -> Model\n",
    "## EvaluationData -> Pre-Process -> Model -> Result\n",
    "\n",
    "## Detection pipeline\n",
    "## [path] --> read -- [image, label] -->  crop -- [image, label]-->  hsv_hog --> [(hsv, hog), label] --> TrainingModule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileSource:\n",
    "    def __init__(self, src_dir, pattern, input_type='file'):\n",
    "        self.src_dir = src_dir\n",
    "        self.pattern = pattern\n",
    "        self.files = glob.glob(os.path.join(self.src_dir + '/**/' + self.pattern), recursive=True)\n",
    "        self.current = 0\n",
    "        self.offset = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def next(self):\n",
    "        if self.current == len(self.files):\n",
    "            raise StopIteration()\n",
    "        else:\n",
    "            file = self.files[self.current]\n",
    "            self.current = self.current + 1\n",
    "            return file\n",
    "    \n",
    "    def has_next(self):\n",
    "        return self.current < len(self.files)\n",
    "    \n",
    "class PickledSource:\n",
    "    \n",
    "    def __init__(self, input_path):\n",
    "        self.done = False\n",
    "        self.input_path = input_path\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def next(self):\n",
    "        if self.done:\n",
    "            raise StopIteration()\n",
    "        else:\n",
    "            with open(self.input_path, 'rb') as inp:\n",
    "                result = pickle.load(inp)\n",
    "                self.done = True\n",
    "                return result\n",
    "    \n",
    "    def has_next(self):\n",
    "        return not self.done\n",
    "    \n",
    "class EmptySource:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.done = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def next(self):\n",
    "        return None\n",
    "    \n",
    "    def has_next(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_DIR = '/Users/raman/work/car_nd/self_driving_nano/p5_vehicle_detection'\n",
    "TEST_INPUT_DIR = os.path.join(BASE_DIR, 'test_images')\n",
    "INPUT_DIR_CARS = os.path.join(BASE_DIR, 'training_data', 'vehicles')\n",
    "INPUT_DIR_OTHERS = os.path.join(BASE_DIR, 'training_data', 'non-vehicles')\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "orient = 9\n",
    "spatial_bining_size = (32, 32)\n",
    "nbins=32\n",
    "image_shape = (760, 1280)\n",
    "\n",
    "\n",
    "def display_image(image):\n",
    "    for image in pipeline.process():\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "import pickle\n",
    "\n",
    "class Operation(metaclass=ABCMeta):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def operate(self, context, inp):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SourceOperator:\n",
    "    \n",
    "    def __init__(self, data_source):\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def next(self):\n",
    "        while self.data_source.has_next():\n",
    "            inp = self.data_source.next()\n",
    "            yield inp\n",
    "            \n",
    "    def eval(self, input_data):\n",
    "        return input_data        \n",
    "        \n",
    "class ProcessingOperator:\n",
    "    \n",
    "    def __init__(self, src, operation):\n",
    "        self.src = src.next()\n",
    "        self.operation = operation\n",
    "        \n",
    "    def next(self):\n",
    "        for inp in self.src:\n",
    "            out =  self.operation.operate(inp)\n",
    "            if out.itemize:\n",
    "                for v in out:\n",
    "                    yield v\n",
    "            else:\n",
    "                yield out.value\n",
    "    \n",
    "    def eval(self, input):\n",
    "        return self.operation.operate(input)\n",
    "            \n",
    "class CompositeOperator:\n",
    "    \n",
    "    def __init__(self, src, operations):\n",
    "        self.src = src.next()\n",
    "        self.operations = operations\n",
    "        \n",
    "    def next(self):\n",
    "        for inp in self.src:\n",
    "            outputs = []\n",
    "            for op in self.operations:\n",
    "                out =  op.operate(inp)\n",
    "                if out.itemize is True:\n",
    "                    for o in out.values:\n",
    "                        outputs.append(out)\n",
    "                else:\n",
    "                    outputs.append(out.value)\n",
    "            yield np.concatenate(outputs)\n",
    "    \n",
    "    def eval(self, input):\n",
    "        for inp in self.src:\n",
    "            outputs = []\n",
    "            for op in self.operations:\n",
    "                out =  op.operate(inp)\n",
    "                outputs.append(out)\n",
    "            return np.concatenate(outputs) \n",
    "        \n",
    "class OutputOperator:\n",
    "    \n",
    "    def __init__(self, src, writer):\n",
    "        self.src = src.next()\n",
    "        self.writer = writer\n",
    "        \n",
    "    def next(self):\n",
    "        all_inputs = []\n",
    "        count = 0\n",
    "        for inp in self.src:\n",
    "            all_inputs.append(inp)\n",
    "            count = count + 1\n",
    "            if count % 500 == 0:\n",
    "                print(\"{} items processed\".format(count))\n",
    "        print(\"{} processed\".format(count))\n",
    "        yield self.writer.operate(all_inputs)\n",
    "        \n",
    "    def eval(self, input):\n",
    "        return self.writer.operate(input)\n",
    "    \n",
    "class Data:\n",
    "    \n",
    "    def __init__(self, value, itemize = False):\n",
    "        self.value = value\n",
    "        self.itemize = itemize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.operations = []\n",
    "        self.last = SourceOperator(EmptySource())\n",
    "    \n",
    "    def add_source(self, source):\n",
    "        self.source = SourceOperator(source)\n",
    "        self.operations.append(self.source)\n",
    "        self.last = self.source\n",
    "    \n",
    "    def add(self, process):\n",
    "        operator = ProcessingOperator(self.last, process)\n",
    "        self.operations.append(operator)\n",
    "        self.last = operator\n",
    "    \n",
    "    def add_output(self, output):\n",
    "        operator = OutputOperator(self.last, output)\n",
    "        self.output_operator = operator\n",
    "        self.operations.append(operator)\n",
    "        self.last = operator\n",
    "    \n",
    "    def add_composite(self, operations):\n",
    "        operator = CompositeOperator(self.last, operations)\n",
    "        self.operations.append(operator)\n",
    "        self.last = operator\n",
    "        \n",
    "    def process(self):\n",
    "        for output in  self.output_operator.next():\n",
    "            yield output           \n",
    "            \n",
    "    def consume(self, input):\n",
    "        data = input\n",
    "        for op in self.operations:\n",
    "            data = op.operation.operate(data)\n",
    "        return data\n",
    "        \n",
    "def execute_pipeline(pipeline, name=\"\"):\n",
    "    output = []\n",
    "    for out in pipeline.process():\n",
    "        output.append(out)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class ColorSpaceConverter(Operation):\n",
    "    \n",
    "    def __init__(self, color_space):\n",
    "        self.color_space = color_space\n",
    "        \n",
    "    def operate(self, image):\n",
    "        if self.color_space != 'BGR':\n",
    "            if self.color_space == 'RGB':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            if self.color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "            elif self.color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_BGR2LUV)\n",
    "            elif self.color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n",
    "            elif self.color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "            elif self.color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_BGR2YCrCb)\n",
    "        else: feature_image = np.copy(image) \n",
    "        return Data(feature_image)\n",
    "\n",
    "class Reader(Operation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def operate(self, input_path):\n",
    "        image = cv2.imread(input_path)\n",
    "        return Data(image)\n",
    "\n",
    "    \n",
    "class Resizer(Operation):\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "   \n",
    "    def operate(self, image):\n",
    "        return Data(cv2.resize(image, self.size))\n",
    "\n",
    "    \n",
    "class ImageDataAmplifier(Operation):\n",
    "    \n",
    "    def __init__(self, amps):\n",
    "        self.amps = amps\n",
    "        self.angle = 5\n",
    "        \n",
    "    def operate(self, image):\n",
    "        flipped = flip(image)\n",
    "        rotated = rotate(image)\n",
    "        return Data([image, flipped, rotated], itemized = True)\n",
    "        \n",
    "    def flip(self, image):\n",
    "        return np.fliplr(image)\n",
    "    \n",
    "    def rotate(self, image):\n",
    "        x,y,c = image.shape\n",
    "        image_center = tuple(np.array(image.shape)/2)\n",
    "        rot_mat = cv2.getRotationMatrix2D((y/2, x/2),angle,1.0),\n",
    "        rotated = cv2.warpAffine(image, rot_mat, (y,x),flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_REPLICATE)\n",
    "        return rotated\n",
    "    \n",
    "        \n",
    "    \n",
    "class SpatialBinning(Operation):\n",
    "    \n",
    "    def __init__(self, size, color_space='RGB'):\n",
    "        self.color_space = color_space\n",
    "        self.size = size\n",
    "    \n",
    "    def operate(self, image):        \n",
    "        # Convert image to new color space (if specified)\n",
    "        if self.color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)             \n",
    "        # Use cv2.resize().ravel() to create the feature vector\n",
    "        features = cv2.resize(feature_image, self.size).ravel() \n",
    "        # Return the feature vector\n",
    "        return Data(features)\n",
    "\n",
    "\n",
    "class ChannelHistogram(Operation):\n",
    "\n",
    "    def __init__(self, nbins=32, bins_range=(0, 256)):\n",
    "        self.nbins = nbins\n",
    "        self.bins_range = bins_range\n",
    "    \n",
    "    def operate(self, image): \n",
    "        return Data(self.color_hist(image))\n",
    "\n",
    "    # Define a function to compute color histogram features  \n",
    "    def color_hist(self, img):\n",
    "        # Compute the histogram of the color channels separately\n",
    "        channel1_hist = np.histogram(img[:,:,0], bins=self.nbins, range=self.bins_range)\n",
    "        channel2_hist = np.histogram(img[:,:,1], bins=self.nbins, range=self.bins_range)\n",
    "        channel3_hist = np.histogram(img[:,:,2], bins=self.nbins, range=self.bins_range)\n",
    "        # Concatenate the histograms into a single feature vector\n",
    "        hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "        # Return the individual histograms, bin_centers and feature vector\n",
    "        return hist_features\n",
    "\n",
    "\n",
    "from skimage.feature import hog\n",
    "class HOG(Operation):\n",
    "    \n",
    "    def __init__(self, orient, pix_per_cell, cell_per_block, feature_vector = True, hog_channel = 1):\n",
    "        self.orient = orient\n",
    "        self.pix_per_cell = pix_per_cell\n",
    "        self.cell_per_block = cell_per_block\n",
    "        self.hog_channel = hog_channel\n",
    "        self.feature_vector = feature_vector\n",
    "    \n",
    "    def operate(self, image):\n",
    "        return Data(self.hog(image))\n",
    "    \n",
    "    def hog(self, image):              \n",
    "        if self.hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(image.shape[2]):\n",
    "                channel_feature = hog(image, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                      cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \n",
    "                      feature_vector=self.feature_vector)\n",
    "                hog_features.append(channel_feature)\n",
    "            hog_features = np.ravel(hog_features)        \n",
    "        else:\n",
    "            hog_features = hog(image[:,:,self.hog_channel], orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                      cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \n",
    "                      feature_vector=self.feature_vector)\n",
    "        return hog_features\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "class StandardScaling:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def operate(self, feature_list):\n",
    "        X = np.vstack(feature_list).astype(np.float64)\n",
    "        # Fit a per-column scaler\n",
    "        X_scaler = StandardScaler().fit(X)\n",
    "        # Apply the scaler to X\n",
    "        scaled_X = X_scaler.transform(X)\n",
    "        return Data(scaled_X)\n",
    "\n",
    "class PickleSerializer(Operation):\n",
    "    \n",
    "    def __init__(self, output_path):\n",
    "        self.output_path = output_path\n",
    "    \n",
    "    def operate(self, inp):\n",
    "        with open(self.output_path, 'wb') as output:\n",
    "            print(\"writing {} items to {}\".format(len(inp), self.output_path))\n",
    "            pickle.dump(inp, output)\n",
    "\n",
    "class ImageDisplay(Operation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def operate(self, inp):\n",
    "        plt.imshow(inp)\n",
    "        plt.show()\n",
    "        \n",
    "class SlidingWindows(Operation):\n",
    "    \n",
    "    def __init__(self, window_size = (64, 64), overlap = (0.5, 0.5), varying_size = False):\n",
    "        self.window_size = window_size\n",
    "        self.varying_size = varying_size\n",
    "        self.overlap = overlap\n",
    "        self.bounding_boxes = None\n",
    "        \n",
    "    def operate(self, image):\n",
    "        if self.bounding_boxes is None:\n",
    "            h, w, _ = image.shape\n",
    "            self.bounding_boxes = get_bounding_boxes((0, w), (h/2, h), self.window_size, self.window_size, self.overlap)\n",
    "        return Data((image, self.bounding_boxes))\n",
    "   \n",
    "    def bounding_boxes(self, x_start_stop, y_start_stop, xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "        # If x and/or y start/stop positions not defined, set to image size\n",
    "        if x_start_stop[0] is None or  x_start_stop[1] is None:\n",
    "            x_start_stop = [0, img.shape[1]]\n",
    "        if y_start_stop[0] is None or  y_start_stop[1] is None:\n",
    "            y_start_stop = [0, img.shape[0]]\n",
    "\n",
    "        # Compute the span of the region to be searched    \n",
    "        # Compute the number of pixels per step in x/y\n",
    "        n_pixels_per_step = (np.int(xy_window[0]*xy_overlap[0]), np.int(xy_window[1]*xy_overlap[1]))\n",
    "        # Compute the number of windows in x/y\n",
    "        n_windows_x = 1 + (x_start_stop[1] - x_start_stop[0] - xy_window[0])/xy_window[0]\n",
    "        n_windows_y = 1 + (y_start_stop[1] - y_start_stop[0] - xy_window[1])/xy_window[1]\n",
    "        # Initialize a list to append window positions to\n",
    "        window_list = []\n",
    "        for x_window_pos in range(x_start_stop[0], x_start_stop[1]-n_pixels_per_step[0], n_pixels_per_step[0]):\n",
    "            for y_window_pos in range(y_start_stop[0], y_start_stop[1] - xy_window[1], n_pixels_per_step[1]):\n",
    "                top_left = (x_window_pos, y_window_pos)\n",
    "                bottom_right = (x_window_pos + xy_window[0], y_window_pos + xy_window[1])\n",
    "                box = (top_left, bottom_right)\n",
    "                window_list.append(box)\n",
    "\n",
    "        return  window_list \n",
    "\n",
    "# resize area of interest\n",
    "\n",
    "class ResizeArea(Operation):\n",
    "    \n",
    "    def __init__(self, ystart, ystop, scale):\n",
    "        self.ystart = ystart\n",
    "        self.ystop = ystop\n",
    "        self.scale = scale\n",
    "        \n",
    "    def operate(self, image):\n",
    "        img_of_interest = image[ystart:ystop,:,:]\n",
    "        resized = img_of_interest\n",
    "        if self.scale != 1:\n",
    "            imshape = img_of_interest.shape\n",
    "            resized = cv2.resize(img_of_interest, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        return Data((image, resized))\n",
    "    \n",
    "    \n",
    "class Hog(Operation):\n",
    "    \n",
    "    def __init__(self, orient, pix_per_cell, cell_per_block):\n",
    "        self.hog_op = HOG(orient, pix_per_cell, cell_per_block, feature_vec = False, channel=1)\n",
    "        \n",
    "    def operate(self, input):\n",
    "        orig_image = input[0]\n",
    "        resized_area = input[1]\n",
    "        \n",
    "        # Compute individual channel HOG features for the entire image\n",
    "        hog_features  = self.hog_op.operate(resized_area) # L Channel only \n",
    "        return Data((orig_image, (resized_image, hog_features)))\n",
    "               \n",
    "class PatternDetector(Operation):\n",
    "    \n",
    "    def __init__(self, model, windows):\n",
    "        self.model = model\n",
    "        self.windows = windows\n",
    "        self.channel_hist_op = ChannelHistogram()\n",
    "        self.spatial_op = SpatialBinning(spatial_bining_size, color_space = 'HLS')\n",
    "\n",
    "    def operate(self, input):\n",
    "        resized_image = input[1][0]\n",
    "        hog_features = input[1][1]\n",
    "        hist_features = self.channel_hist_op.operate(resized_image)\n",
    "        spatial_features = self.spatial_op.operate(resized_image)\n",
    "        \n",
    "        ytop = 0\n",
    "        for window in windows:\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(resized_image[0:0+window[0], 0:window[1]], (64,64))\n",
    "            # Get color features\n",
    "            hist_features = self.channel_hist_op.operate(subimg)\n",
    "            spatial_features = self.channel_hist_op(subimg)\n",
    "            \n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            test_prediction = self.model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 items processed\n",
      "1000 items processed\n",
      "1500 items processed\n",
      "2000 items processed\n",
      "2500 items processed\n",
      "3000 items processed\n",
      "3500 items processed\n",
      "4000 items processed\n",
      "4500 items processed\n",
      "5000 items processed\n",
      "5500 items processed\n",
      "6000 items processed\n",
      "6500 items processed\n",
      "7000 items processed\n",
      "7500 items processed\n",
      "8000 items processed\n",
      "8500 items processed\n",
      "8792 processed\n",
      "writing 8792 items to feature_vectors_cars_4.p\n",
      "500 items processed\n",
      "1000 items processed\n",
      "1500 items processed\n",
      "2000 items processed\n",
      "2500 items processed\n",
      "3000 items processed\n",
      "3500 items processed\n",
      "4000 items processed\n",
      "4500 items processed\n",
      "5000 items processed\n",
      "5500 items processed\n",
      "6000 items processed\n",
      "6500 items processed\n",
      "7000 items processed\n",
      "7500 items processed\n",
      "8000 items processed\n",
      "8500 items processed\n",
      "8968 processed\n",
      "writing 8968 items to feature_vectors_non_cars_4.p\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "target_resize = (32, 32)\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "orient = 9\n",
    "spatial_bining_size = (32, 32)\n",
    "output_file_cars = 'feature_vectors_cars_4.p'\n",
    "output_file_others = 'feature_vectors_non_cars_4.p'\n",
    "\n",
    "def extract_feature_vectors(source, output_file):\n",
    "    pipeline = Pipeline()\n",
    "    pipeline.add_source(source)\n",
    "    pipeline.add(Reader())\n",
    "    pipeline.add(Resizer(target_resize))\n",
    "    pipeline.add(ColorSpaceConverter('HSV'))\n",
    "    pipeline.add_composite([ChannelHistogram(), SpatialBinning(spatial_bining_size),\n",
    "                                                               HOG(orient, cell_per_block, pix_per_cell)])\n",
    "    pipeline.add(StandardScaling())\n",
    "    pipeline.add_output(PickleSerializer(output_file))\n",
    "    return pipeline\n",
    "\n",
    "source_cars     = FileSource(INPUT_DIR_CARS, \"*.png\")\n",
    "pipeline = extract_feature_vectors(source_cars, output_file_cars)\n",
    "execute_pipeline(pipeline, 'car vectors')\n",
    "\n",
    "source_non_cars = FileSource(INPUT_DIR_OTHERS, \"*.png\")\n",
    "pipeline = extract_feature_vectors(source_non_cars, output_file_others)\n",
    "execute_pipeline(pipeline, 'non-car vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-720c0ce15b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcar_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnon_car_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_cars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cars has {} items\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcar_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"non_cars has {} items\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_car_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cars' is not defined"
     ]
    }
   ],
   "source": [
    "car_features = cars[0]\n",
    "non_car_features = non_cars[0]\n",
    "\n",
    "print(\"cars has {} items\".format(len(car_features)))\n",
    "print(\"non_cars has {} items\".format(len(non_car_features)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cars has 8792 items\n",
      "non_cars has 8968 items\n",
      "8792\n"
     ]
    }
   ],
   "source": [
    "output_file_cars = 'feature_vectors_cars_3.p'\n",
    "output_file_others = 'feature_vectors_non_cars_3.p'\n",
    "\n",
    "with open(output_file_cars, 'rb') as inp:\n",
    "    car_features = pickle.load(inp)\n",
    "\n",
    "with open(output_file_others, 'rb') as inp:\n",
    "    non_car_features = pickle.load(inp)\n",
    "\n",
    "print(\"cars has {} items\".format(len(car_features)))\n",
    "print(\"non_cars has {} items\".format(len(non_car_features)))\n",
    "\n",
    "car_features = [cp.flatten() for cp in car_features]\n",
    "non_car_features = [cp.flatten() for cp in non_car_features]\n",
    "\n",
    "print (len(car_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of SVC =  0.944538288288\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, non_car_features)).astype(np.float64)\n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define a labels vector based on features lists\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(non_car_features))))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "# Use a linear SVC (support vector classifier)\n",
    "svc = LinearSVC()\n",
    "# Train the SVC\n",
    "svc.fit(X_train, y_train)\n",
    "print('Test Accuracy of SVC = ', svc.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_file = 'svc_classifier.p'\n",
    "with open(model_file, 'wb') as out:\n",
    "    pickle.dump(svc, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of SVC =  0.944538288288\n"
     ]
    }
   ],
   "source": [
    "with open(model_file, 'rb') as inp:\n",
    "    model = pickle.load(inp)\n",
    "\n",
    "print('Test Accuracy of SVC = ', model.score(X_test, y_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video project_5_output.mp4\n",
      "[MoviePy] Writing video project_5_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [01:28<00:00, 12.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: project_5_output.mp4 \n",
      "\n",
      "CPU times: user 26.6 s, sys: 3.22 s, total: 29.8 s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "## define video processing pipeline\n",
    "# initialize variable-size bounding windows\n",
    "# extract window images\n",
    "# compute HOG + HSV\n",
    "# image -> (image, boxes) -> (image, features(HSV, HOG), boxes) -> (image, boxes, car_no_car)-> image\n",
    "pipeline = Pipeline()\n",
    "pipeline.add(ColorSpaceConverter('HLS'))\n",
    "pipeline.add()\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "output = 'project_5_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "output_clip = clip1.fl_image(pipeline.consume)\n",
    "%time output_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Vehicle_Detector:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.ystart = config['ystart']\n",
    "        self.ystop = config['ystop']\n",
    "        self.scale= config['scale']\n",
    "        self.svc = config['svc']\n",
    "        self.X_scaler = config['X_scaler']\n",
    "        self.orient = config['orient']\n",
    "        self.pix_per_cell = config['pix_per_cell']\n",
    "        self.cell_per_block = config['cell_per_block']\n",
    "        self.spatial_size = config['spatial_size']\n",
    "        self.hist_bins = config['hist_bins']\n",
    "        \n",
    "    @staticmethod        \n",
    "    def compute_sliding_windows(x_start_stop, y_start_stop, xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "     # If x and/or y start/stop positions not defined, set to image size\n",
    "        if x_start_stop[0] is None or  x_start_stop[1] is None:\n",
    "            x_start_stop = [0, img.shape[1]]\n",
    "        if y_start_stop[0] is None or  y_start_stop[1] is None:\n",
    "            y_start_stop = [0, img.shape[0]]\n",
    "\n",
    "        # Compute the span of the region to be searched    \n",
    "        # Compute the number of pixels per step in x/y\n",
    "        n_pixels_per_step = (np.int(xy_window[0]*xy_overlap[0]), np.int(xy_window[1]*xy_overlap[1]))\n",
    "        # Compute the number of windows in x/y\n",
    "        n_windows_x = 1 + (x_start_stop[1] - x_start_stop[0] - xy_window[0])/xy_window[0]\n",
    "        n_windows_y = 1 + (y_start_stop[1] - y_start_stop[0] - xy_window[1])/xy_window[1]\n",
    "        # Initialize a list to append window positions to\n",
    "        window_list = []\n",
    "        for x_window_pos in range(x_start_stop[0], x_start_stop[1]-n_pixels_per_step[0], n_pixels_per_step[0]):\n",
    "            for y_window_pos in range(y_start_stop[0], y_start_stop[1] - xy_window[1], n_pixels_per_step[1]):\n",
    "                top_left = (x_window_pos, y_window_pos)\n",
    "                bottom_right = (x_window_pos + xy_window[0], y_window_pos + xy_window[1])\n",
    "                box = (top_left, bottom_right)\n",
    "                window_list.append(box)\n",
    "\n",
    "        return  window_list \n",
    "\n",
    "\n",
    "    # Define a single function that can extract features using hog sub-sampling and make predictions\n",
    "    def find_cars(self, img):\n",
    "        draw_img = np.copy(img)\n",
    "        img = img.astype(np.float32)/255\n",
    "        print(\"{} {}\".format(self.ystart, self.ystop))\n",
    "        img_tosearch = img[self.ystart:self.ystop,:,:]\n",
    "        ctrans_tosearch = convert_color(img_tosearch, conv='RGB2YCrCb')\n",
    "        if self.scale != 1:\n",
    "            imshape = ctrans_tosearch.shape\n",
    "            ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/self.scale), np.int(imshape[0]/self.scale)))\n",
    "\n",
    "        ch1 = ctrans_tosearch[:,:,0]\n",
    "        ch2 = ctrans_tosearch[:,:,1]\n",
    "        ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "        # Define blocks and steps as above\n",
    "        nxblocks = (ch1.shape[1] // self.pix_per_cell) - self.cell_per_block + 1\n",
    "        nyblocks = (ch1.shape[0] // self.pix_per_cell) - self.cell_per_block + 1 \n",
    "        nfeat_per_block = self.orient*self.cell_per_block**2\n",
    "\n",
    "        # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "        window = 64\n",
    "        nblocks_per_window = (window // self.pix_per_cell) - self.cell_per_block + 1\n",
    "        cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "        nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "        nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "\n",
    "        # Compute individual channel HOG features for the entire image\n",
    "        hog1 = get_hog_features(ch1, self.orient, self.pix_per_cell, self.cell_per_block, feature_vec=False)\n",
    "        hog2 = get_hog_features(ch2, self.orient, self.pix_per_cell, self.cell_per_block, feature_vec=False)\n",
    "        hog3 = get_hog_features(ch3, self.orient, self.pix_per_cell, self.cell_per_block, feature_vec=False)\n",
    "\n",
    "        for xb in range(nxsteps):\n",
    "            for yb in range(nysteps):\n",
    "                ypos = yb*cells_per_step\n",
    "                xpos = xb*cells_per_step\n",
    "                # Extract HOG for this patch\n",
    "                hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "                hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "                hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "                hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "                xleft = xpos*self.pix_per_cell\n",
    "                ytop = ypos*self.pix_per_cell\n",
    "\n",
    "                # Extract the image patch\n",
    "                subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "\n",
    "                # Get color features\n",
    "                spatial_features = bin_spatial(subimg, size=self.spatial_size)\n",
    "                hist_features = color_hist(subimg, nbins=self.hist_bins)\n",
    "\n",
    "                # Scale features and make a prediction\n",
    "                test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "                #test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))    \n",
    "                test_prediction = svc.predict(test_features)\n",
    "\n",
    "                if test_prediction == 1:\n",
    "                    xbox_left = np.int(xleft*self.scale)\n",
    "                    ytop_draw = np.int(ytop*self.scale)\n",
    "                    win_draw = np.int(window*self.scale)\n",
    "                    cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "\n",
    "        return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import hog\n",
    "\n",
    "def convert_color(img, conv='RGB2YCrCb'):\n",
    "    if conv == 'RGB2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    if conv == 'BGR2YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
    "    if conv == 'RGB2LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=False, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=False, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "                        \n",
    "def color_hist(img, nbins=32):    #bins_range=(0, 256)\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "orient = 9\n",
    "spatial_bining_size = (32, 32)\n",
    "nbins=32\n",
    "image_shape = (760, 1280)\n",
    "\n",
    "config = {}\n",
    "config['ystart'] = np.int(image_shape[0]/2)\n",
    "config['ystop']   = image_shape[0]\n",
    "config['scale']   = 1\n",
    "config['svc']     = model\n",
    "config['X_scaler'] = X_scaler\n",
    "config['orient'] = orient\n",
    "config['pix_per_cell'] = pix_per_cell\n",
    "config['cell_per_block'] = cell_per_block\n",
    "config['spatial_size'] = spatial_bining_size\n",
    "config['hist_bins'] = nbins\n",
    "      \n",
    "detector = Vehicle_Detector(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 760\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,8460) (3492,) (1,8460) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-4f2cbae1d661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'project_5_output.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclip1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"project_video.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_cars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time output_clip.write_videofile(output, audio=False)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mfl_image\u001b[0;34m(self, image_func, apply_to)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mfl\u001b[0;34m(self, fun, apply_to, keep_duration)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-181>\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36moutplace\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"\"\" Applies f(clip.copy(), *a, **k) and returns clip.copy()\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36mset_make_frame\u001b[0;34m(self, mf)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \"\"\"\n\u001b[1;32m    693\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-138>\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(f, *a, **kw)\u001b[0m\n\u001b[1;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[1;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m#mf = copy(self.make_frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mnewclip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gf, t)\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \"\"\"\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;31m# --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-f9a5cee311ea>\u001b[0m in \u001b[0;36mfind_cars\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;31m# Scale features and make a prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhog_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;31m#test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mtest_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/carnd/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,8460) (3492,) (1,8460) "
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "output = 'project_5_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "output_clip = clip1.fl_image(detector.find_cars)\n",
    "%time output_clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
